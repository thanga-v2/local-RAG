{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f24af2-3777-4bd8-afe5-54bb4131145a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building a RAG(Retrieval Augmented Generation) pipeline from scratch \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e56e0604-557b-4d03-acd7-85961c77e5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# installing and verifying torch manually\n",
    "\n",
    "# pip3 install torch torchvision\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc7ef861-46b7-405e-a924-82d6f689dc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84cfaada-4bef-4026-aa31-18022f0b3f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "var = torch.rand(10,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e31b6951-a71b-452c-be94-552ac8796de9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5896, 0.7405, 0.4318, 0.6794, 0.9521, 0.0934, 0.5035, 0.6383, 0.9757,\n",
       "         0.0490, 0.4231, 0.7973, 0.2975, 0.6313, 0.7023, 0.0090, 0.2655, 0.3913,\n",
       "         0.9591, 0.0300],\n",
       "        [0.7601, 0.4640, 0.7692, 0.7797, 0.5399, 0.7324, 0.4939, 0.7826, 0.3750,\n",
       "         0.9877, 0.9441, 0.4140, 0.7683, 0.5377, 0.3388, 0.7754, 0.4934, 0.4358,\n",
       "         0.1821, 0.7938],\n",
       "        [0.2906, 0.1608, 0.7488, 0.0713, 0.2377, 0.1399, 0.2662, 0.4450, 0.2013,\n",
       "         0.1799, 0.0763, 0.8409, 0.4124, 0.5131, 0.3840, 0.9787, 0.8420, 0.4047,\n",
       "         0.7416, 0.5673],\n",
       "        [0.9478, 0.5520, 0.1299, 0.4541, 0.6727, 0.2297, 0.4384, 0.3955, 0.8875,\n",
       "         0.9005, 0.2569, 0.8001, 0.6537, 0.5904, 0.8617, 0.7072, 0.4580, 0.2755,\n",
       "         0.7071, 0.8985],\n",
       "        [0.4682, 0.7900, 0.5041, 0.7696, 0.0892, 0.6577, 0.5213, 0.9373, 0.6070,\n",
       "         0.1511, 0.6237, 0.2077, 0.2918, 0.7980, 0.5563, 0.3488, 0.6350, 0.9124,\n",
       "         0.6287, 0.8722],\n",
       "        [0.4791, 0.2203, 0.7109, 0.3883, 0.6416, 0.2433, 0.3124, 0.3279, 0.1184,\n",
       "         0.6307, 0.0522, 0.1916, 0.1990, 0.1083, 0.1957, 0.1726, 0.4103, 0.3950,\n",
       "         0.0739, 0.3295],\n",
       "        [0.8321, 0.6444, 0.5881, 0.9385, 0.4589, 0.8920, 0.1024, 0.3589, 0.6420,\n",
       "         0.6324, 0.0106, 0.7253, 0.8751, 0.4552, 0.7867, 0.4877, 0.4253, 0.3169,\n",
       "         0.8972, 0.8848],\n",
       "        [0.7386, 0.8404, 0.9552, 0.0313, 0.9432, 0.9485, 0.1132, 0.3762, 0.1233,\n",
       "         0.2654, 0.3587, 0.2917, 0.8485, 0.7483, 0.8293, 0.9863, 0.9218, 0.5279,\n",
       "         0.2713, 0.1805],\n",
       "        [0.2606, 0.5575, 0.7681, 0.1281, 0.1017, 0.7844, 0.8810, 0.1646, 0.6922,\n",
       "         0.5666, 0.7455, 0.6732, 0.2092, 0.2277, 0.7192, 0.2074, 0.3333, 0.2151,\n",
       "         0.7489, 0.8279],\n",
       "        [0.5996, 0.1410, 0.0441, 0.5126, 0.0920, 0.9572, 0.7673, 0.0049, 0.8137,\n",
       "         0.9564, 0.4992, 0.8320, 0.2441, 0.3423, 0.1559, 0.1321, 0.9087, 0.4402,\n",
       "         0.3353, 0.2491]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d58aeef-8c78-4e3b-9ad0-7f7181d0ae74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG takes information from one place and pass it to LLM so it can generate outputs based on that information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59c21b9b-4f76-4c68-90e5-645ef771184b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### What is RAG ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e995b98-22a8-4d2d-894b-637ed792adc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieval - Find relevant information given a query e.g \"what are macro nutrients and what do they do\"? -> this retieves passage of text relevant \n",
    "# to macro nutrients from the text book (pdf source which we have added here)\n",
    "\n",
    "# Augmented - we want to take relevant information and augment our input(prompt) to an LLM with relevant information.\n",
    "\n",
    "# Generation - Take the first two steps and pass it to the LLM for generative outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c883456-90d3-498d-940a-075a31576e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Main goal of RAG is to improve generation of outputs in LLM.\n",
    "\n",
    "# 1. prevent hallucination - LLM are good at generating good looking text. but it doesn't mean that the data which we retrieved are factual.\n",
    "# RAG can help LLMs generate information based on relevant passages that are factual.\n",
    "\n",
    "# 2. Work with custom data - LLM's are trained with internet-scale data. \n",
    "# which means lots of the responses are generic.\n",
    "# RAG helps to creeate specific responses based on specific documents (e.g your own companies support documents)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "def9a24d-32ba-4e77-8460-522d935b968b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### How RAG can be useful ?\n",
    "\n",
    "\n",
    "# customer support Q&A -> Treat the sources and when some one asks a specific question it should be able to retrieve a \n",
    "# relevant information / passage and have an LLM to craft those snippets into an answer/ ex: chatbot for your documentation\n",
    "\n",
    "# Email chain analysis -> take some unstructured data from chain of emails and have an LLM to transform into structured data\n",
    "\n",
    "# company internal documentation chat \n",
    "\n",
    "# Textbook q&a\n",
    "\n",
    "\n",
    "\n",
    "# common : take your relevant documents to a query and process them with LLMs\n",
    "\n",
    "# we can consider LLM as a calculator for words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5d4201f-2a53-4158-9ce1-513a6b8b5edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### why we need local RAG ?\n",
    "\n",
    "# privacy, speed, cost not dependant on vendors like open AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed83ebe-1fb5-48a8-b538-3e1607c7a464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # workflow\n",
    "\n",
    "# pdf -> pre process the text to smaller chunks -> smaller chunks (thi is our context) -> embedding model (turns text/query into numerical respresntation) ->\n",
    "# store in pytorch tesnsor \n",
    "\n",
    "\n",
    "# RAG paper -> https://arxiv.org/pdf/2312.10997.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38ce011a-56b2-494d-9c2a-42faee905b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# this is called document preprocessing and embedding creation\n",
    "\n",
    "# open a pdf document\n",
    "# format the text of pdf ready for an embedding model (in this case its called as chunks)\n",
    "# embed all of the chunks and turn it into a numerical representation which can store for later\n",
    "\n",
    "\n",
    "# this is for search and answer\n",
    "\n",
    "\n",
    "# build a retrieval system that uses vector search to find the relevant chunk of text based on a query\n",
    "# create a prompt that incorporates the retrieved pieces of text\n",
    "# generate an answer to the query based on the passages of the text book with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f6d02a-474c-47e6-9172-bde8311e2702",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "### document preprocessing and embedding creation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d021af9-c58f-4b80-a0d6-87edb6b40a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingredients :\n",
    "# pdf document of choice\n",
    "# embedding model of choice\n",
    "\n",
    "# steps :\n",
    "# import pdf document\n",
    "# process text for embedding (split into chunk of sentences)\n",
    "# embed text chunks with embedding model\n",
    "# save embedding to file for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54aa60c9-2e71-4b1e-a722-c330e4eb73c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "\n",
    "# get the pdf document path \n",
    "\n",
    "pdf_path = \"Human-Nutrition-2020-Edition-1598491699.pdf\"\n",
    "\n",
    "# check\n",
    "print(os.path.exists(pdf_path))\n",
    "\n",
    "# download PDF\n",
    "\n",
    "if not os.path.exists(pdf_path):\n",
    "    print(\"doesn't exist\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57653764-2678-4893-a2d4-fd1babcedf34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human-Nutrition-2020-Edition-1598491699.pdf'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
